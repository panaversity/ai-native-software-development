# Detailed Evaluation Rubric

This document provides comprehensive tier criteria for evaluating educational content across 6 weighted categories.

---

## Category 1: Technical Accuracy (30% weight)

### Excellent (90-100%)
**Code Quality:**
- All code examples are syntactically correct and execute without errors
- Complete type hints for all functions, parameters, and return values
- Follows project code standards (PEP 8 or equivalent) consistently
- Includes error handling where appropriate
- Code is production-quality, not just "demo code"

**Explanations:**
- Technical concepts explained with precision and depth
- Explanations are factually accurate with no misleading statements
- Complex concepts broken down clearly with accurate analogies
- Edge cases and gotchas explicitly addressed
- No outdated or deprecated information

**Examples:**
- 3-5 runnable, self-contained code examples provided
- Examples progressively increase in complexity
- Each example demonstrates specific concept clearly
- Examples include inline comments explaining key lines
- All examples have been tested and verified to work

**Evidence:**
- Cite specific code blocks that exemplify quality
- Note any technical review or validation performed
- Reference accuracy of technical terminology used

---

### Good (75-89%)
**Code Quality:**
- All code examples execute correctly
- Type hints present for most functions (≥80% coverage)
- Generally follows code standards with minor inconsistencies
- Basic error handling included
- Code is functional and instructive

**Explanations:**
- Technical concepts explained accurately
- Explanations are clear and understandable
- Key concepts well-covered, minor details may be simplified
- Most important edge cases addressed
- Information is current and relevant

**Examples:**
- 2-4 working code examples provided
- Examples demonstrate key concepts
- Complexity progression is present
- Most examples include helpful comments
- Examples are verified to work

**Evidence:**
- Note which technical elements meet standards
- Identify minor gaps or areas for polish
- Confirm core technical accuracy

---

### Needs Work (50-74%)
**Code Quality:**
- Some code examples have minor bugs or syntax errors
- Type hints incomplete (<80% coverage) or missing
- Code standards inconsistently applied
- Limited or no error handling
- Code works but lacks polish

**Explanations:**
- Technical concepts generally accurate but may have gaps
- Some explanations unclear or imprecise
- Important concepts under-explained
- Edge cases not consistently addressed
- Some outdated or less-than-ideal approaches used

**Examples:**
- 1-2 code examples provided, or examples too simple
- Examples demonstrate concepts but lack depth
- Limited complexity progression
- Minimal or no code comments
- Not all examples verified to work

**Evidence:**
- List specific technical errors or omissions
- Note which concepts need deeper explanation
- Identify missing or weak examples

---

### Insufficient (<50%)
**Code Quality:**
- Code examples have significant errors or don't run
- Type hints largely missing (<50% coverage)
- Code standards not followed
- No error handling
- Code appears incomplete or hastily written

**Explanations:**
- Technical inaccuracies or misleading statements present
- Explanations confusing or incorrect
- Key concepts missing or poorly explained
- Edge cases ignored
- Outdated or deprecated approaches recommended

**Examples:**
- No examples, or examples don't work
- Examples don't clearly demonstrate concepts
- No progression in complexity
- No code comments
- Examples not tested

**Evidence:**
- Document critical technical errors
- List missing or incorrect concepts
- Note lack of working examples

---

## Category 2: Pedagogical Effectiveness (25% weight)

### Excellent (90-100%)
**Learning Design:**
- Clear, measurable learning objectives stated upfront (3-4 per lesson)
- Learning objectives follow Bloom's taxonomy appropriately
- All stated learning objectives fully addressed in content
- Strong alignment between objectives, content, and assessments
- Appropriate cognitive load for target audience

**Concept Scaffolding:**
- Exemplary "show-then-explain" pattern throughout
- Concepts introduced in logical, progressive sequence
- 3-4 core concepts per lesson (optimal cognitive load)
- Each concept builds naturally on previous concepts
- Smooth transitions between concepts with clear connections
- Concrete examples precede abstract explanations consistently

**Exercises & Practice:**
- 3-5 well-designed exercises that reinforce learning objectives
- Exercises range from simple recall to applied problem-solving
- Each exercise includes clear prompts and model solutions
- Exercises encourage "learning WITH AI" (not generating FROM AI)
- Practice problems are authentic and relevant to learners

**Assessment:**
- 2-4 Quick Check questions embedded in content
- Assessments directly test learning objectives
- Mix of question types (multiple choice, scenario-based, reflection)
- Model answers provided with explanations
- Self-assessment opportunities for learners

**Evidence:**
- Quote examples showing strong scaffolding
- Note how exercises align with learning objectives
- Cite specific show-then-explain instances

---

### Good (75-89%)
**Learning Design:**
- Learning objectives stated clearly (2-4 per lesson)
- Objectives generally follow Bloom's taxonomy
- Most learning objectives addressed in content
- Good alignment between objectives and content
- Cognitive load appropriate for most learners

**Concept Scaffolding:**
- "Show-then-explain" pattern generally followed
- Concepts introduced in logical order
- 3-5 concepts per lesson (acceptable range)
- Concepts build on each other with minor gaps
- Most transitions are clear
- Examples usually precede explanations

**Exercises & Practice:**
- 2-4 exercises that relate to learning objectives
- Exercises include mix of difficulty levels
- Clear prompts provided, most have model solutions
- Some guidance on learning WITH AI present
- Practice problems are relevant

**Assessment:**
- 1-3 Quick Check questions included
- Assessments test key concepts
- Some variety in question types
- Model answers provided
- Basic self-assessment available

**Evidence:**
- Note areas where scaffolding is strong
- Identify which exercises are most effective
- Confirm objectives are generally met

---

### Needs Work (50-74%)
**Learning Design:**
- Learning objectives present but may be vague (1-3 per lesson)
- Objectives don't consistently follow Bloom's taxonomy
- Some learning objectives not fully addressed
- Partial alignment between objectives and content
- Cognitive load may be too high or too low

**Concept Scaffolding:**
- "Show-then-explain" pattern inconsistently applied
- Concept order sometimes unclear or jumps around
- More than 5 concepts or fewer than 2 (suboptimal load)
- Some concepts don't clearly build on previous material
- Transitions occasionally abrupt or missing
- Examples and explanations sometimes in wrong order

**Exercises & Practice:**
- 1-2 exercises, or exercises too simple/complex
- Limited range of difficulty
- Some exercises lack clear prompts or solutions
- Limited or no AI learning guidance
- Practice problems may feel contrived

**Assessment:**
- 0-1 Quick Check questions, or questions too vague
- Assessments partially aligned with content
- Limited question variety
- Some model answers missing
- Self-assessment opportunities limited

**Evidence:**
- List specific scaffolding gaps
- Note which learning objectives are not fully met
- Identify weak or missing exercises

---

### Insufficient (<50%)
**Learning Design:**
- Learning objectives missing, vague, or not measurable
- Objectives don't follow Bloom's taxonomy
- Learning objectives not addressed in content
- Poor alignment between objectives and content
- Cognitive load inappropriate for audience

**Concept Scaffolding:**
- "Show-then-explain" pattern not followed
- Concepts introduced without clear order or logic
- Cognitive overload (too many concepts) or underwhelming
- Concepts don't build on each other
- Missing or confusing transitions
- Explanations precede examples (backwards pattern)

**Exercises & Practice:**
- No exercises, or exercises don't relate to content
- No range in difficulty
- Exercises lack prompts or solutions
- No AI learning guidance
- Practice problems are irrelevant or confusing

**Assessment:**
- No Quick Checks or assessments
- Assessments don't align with learning objectives
- No variety in question types
- Model answers missing
- No self-assessment opportunities

**Evidence:**
- Document missing or inadequate learning design elements
- List learning objectives not addressed
- Note lack of exercises or assessments

---

## Category 3: Writing Quality (20% weight)

### Excellent (90-100%)
**Readability:**
- Flesch-Kincaid Grade Level: 7-9 (optimal for target audience)
- Average sentence length: 12-18 words
- Paragraphs: 3-5 sentences maximum (scannable)
- Complex ideas broken into digestible chunks
- Text flows naturally and engages reader

**Clarity & Precision:**
- Every technical term defined on first use with clear explanations
- No jargon without accessible analogies
- Active voice used consistently (≥90% of sentences)
- Direct address ("you," "your") used throughout
- No ambiguous pronouns or unclear references

**Voice & Tone:**
- Engaging, conversational tone that encourages learners
- Professional yet approachable (not condescending)
- Encouraging without being patronizing
- Enthusiasm for topic evident
- Appropriate balance of formality and accessibility

**Inclusive Language:**
- No gatekeeping language ("simple," "just," "obviously," "merely")
- Assumes no prior knowledge unless explicitly stated as prerequisite
- Examples and scenarios inclusive and diverse
- Avoids assumptions about learner background

**Evidence:**
- Cite specific passages demonstrating excellent writing
- Note readability metrics if available
- Quote examples of clear, engaging voice

---

### Good (75-89%)
**Readability:**
- Flesch-Kincaid Grade Level: 7-10 (appropriate range)
- Average sentence length: 12-20 words
- Most paragraphs are 3-5 sentences
- Complex ideas generally well-explained
- Text is clear and understandable

**Clarity & Precision:**
- Most technical terms defined on first use
- Limited unexplained jargon
- Active voice used frequently (≥75% of sentences)
- Direct address generally used
- Few ambiguous references

**Voice & Tone:**
- Professional and clear tone
- Generally approachable
- Mostly encouraging
- Tone appropriate for audience
- Balance of formal and accessible mostly maintained

**Inclusive Language:**
- Minimal gatekeeping language
- Generally accessible to target audience
- Examples mostly inclusive
- Few assumptions about background

**Evidence:**
- Note strong writing passages
- Identify minor areas for improvement
- Confirm overall clarity

---

### Needs Work (50-74%)
**Readability:**
- Flesch-Kincaid Grade Level: 10-12 (somewhat difficult)
- Average sentence length: 20-25 words (long)
- Some paragraphs too long (6+ sentences)
- Complex ideas not always well-explained
- Text occasionally dense or confusing

**Clarity & Precision:**
- Some technical terms not defined on first use
- Jargon used without sufficient explanation
- Active voice inconsistent (50-75% of sentences)
- Direct address inconsistent
- Some ambiguous references or pronouns

**Voice & Tone:**
- Tone inconsistent (sometimes too formal, sometimes too casual)
- Occasionally disengaging or dry
- Limited encouragement
- May feel impersonal
- Balance of formality uneven

**Inclusive Language:**
- Some gatekeeping language present
- Makes some assumptions about prior knowledge
- Limited diversity in examples
- Some exclusionary assumptions

**Evidence:**
- List specific passages needing revision
- Note readability concerns
- Identify tone inconsistencies

---

### Insufficient (<50%)
**Readability:**
- Flesch-Kincaid Grade Level: 12+ (too difficult)
- Average sentence length: 25+ words (very long)
- Many paragraphs too long (7+ sentences)
- Complex ideas poorly explained or not explained
- Text frequently confusing or unclear

**Clarity & Precision:**
- Technical terms regularly undefined
- Heavy use of unexplained jargon
- Passive voice predominant (<50% active voice)
- Indirect address or inconsistent perspective
- Frequent ambiguous references

**Voice & Tone:**
- Tone inappropriate for audience (too formal, condescending, or too casual)
- Disengaging or boring
- Discouraging or intimidating
- Impersonal
- No balance between formal and accessible

**Inclusive Language:**
- Frequent gatekeeping language throughout
- Assumes significant prior knowledge not stated as prerequisite
- Non-inclusive examples
- Exclusionary assumptions common

**Evidence:**
- Document serious writing quality issues
- List passages requiring major revision
- Note lack of accessibility

---

## Category 4: Structure & Organization (15% weight)

### Excellent (90-100%)
**Document Structure:**
- Clear YAML frontmatter with sidebar_position (if applicable), title, and duration
- H1 title matches frontmatter title exactly
- Proper heading hierarchy (H2 for main sections, H3 for subsections)
- No skipped heading levels (H2 → H4)
- 5-8 well-organized main sections

**Opening Hook:**
- Compelling opening that engages immediately (2-3 paragraphs)
- Uses pattern recognition, story, question, or surprising statistic
- Captures attention before diving into technical content
- Clearly establishes relevance and importance
- Smooth transition from hook to main content

**Content Flow:**
- Logical progression from simple to complex
- Smooth transitions between sections with clear signposting
- Each section builds naturally on previous sections
- No jarring jumps in complexity or topic
- Cohesive narrative throughout

**Word Count:**
- Technical lessons: 2,000-2,500 words (8-12 min reading time)
- Conceptual sections: 1,200-2,500 words (appropriate for content type)
- Proper balance - not too brief, not overwhelming
- Depth appropriate for topic complexity

**Closing:**
- Strong closing with reflection prompt or transition
- Clear forward bridge to next lesson/section
- Optional summary when value-adding (not formulaic bullet points)
- Leaves reader with clear takeaways and motivation

**Evidence:**
- Note excellent structural elements
- Quote effective transitions
- Cite engaging opening and closing

---

### Good (75-89%)
**Document Structure:**
- YAML frontmatter present with required fields
- H1 title matches frontmatter
- Heading hierarchy generally correct
- Few if any skipped heading levels
- 4-7 well-organized sections

**Opening Hook:**
- Clear opening that establishes context (2-4 paragraphs)
- Uses one or more engagement techniques
- Captures attention reasonably well
- Establishes relevance
- Adequate transition to main content

**Content Flow:**
- Generally logical progression
- Most transitions are clear
- Sections mostly build on each other
- Few awkward jumps
- Mostly cohesive

**Word Count:**
- Technical lessons: 1,800-2,700 words (reasonable range)
- Conceptual sections: 1,000-2,700 words (acceptable)
- Generally appropriate length
- Depth mostly appropriate

**Closing:**
- Adequate closing with some form of transition or reflection
- Some forward momentum to next content
- Summary present if appropriate
- Leaves reader with some clear takeaways

**Evidence:**
- Note structural strengths
- Identify minor flow issues
- Confirm appropriate length

---

### Needs Work (50-74%)
**Document Structure:**
- YAML frontmatter incomplete or missing fields
- H1 title may not match frontmatter exactly
- Heading hierarchy has some errors
- Some skipped heading levels
- Fewer than 4 or more than 8 sections (too few/many)

**Opening Hook:**
- Opening present but not engaging (1-2 paragraphs or very long)
- Limited use of engagement techniques
- Doesn't strongly establish relevance
- Abrupt transition to main content
- Hook feels generic or weak

**Content Flow:**
- Progression sometimes unclear
- Some transitions missing or abrupt
- Some sections don't clearly build on previous material
- Several awkward jumps in complexity or topic
- Cohesion inconsistent

**Word Count:**
- Technical lessons: 1,500-1,800 or 2,700-3,000 words (too short/long)
- Conceptual sections: 800-1,000 or 2,700-3,000 words (too short/long)
- Length somewhat inappropriate for content
- Depth uneven

**Closing:**
- Closing present but weak or abrupt
- Limited forward momentum
- Summary formulaic or missing when needed
- Takeaways unclear

**Evidence:**
- List structural issues
- Note missing or weak transitions
- Identify length concerns

---

### Insufficient (<50%)
**Document Structure:**
- YAML frontmatter missing or severely incomplete
- H1 title missing or doesn't match frontmatter
- Heading hierarchy incorrect throughout
- Multiple skipped heading levels
- Poorly organized sections (too few, too many, or unclear)

**Opening Hook:**
- No clear opening hook (jumps straight to content)
- No engagement techniques used
- Doesn't establish relevance or importance
- No transition - starts mid-thought
- Opening is confusing or missing

**Content Flow:**
- No clear logical progression
- Transitions frequently missing
- Sections feel disconnected
- Frequent jarring jumps
- Lacks cohesion

**Word Count:**
- Technical lessons: <1,500 or >3,000 words (too short/too long)
- Conceptual sections: <800 or >3,000 words (too short/too long)
- Length highly inappropriate
- Depth severely inadequate or overwhelming

**Closing:**
- No real closing or conclusion
- No forward momentum
- Missing summary when critical
- No clear takeaways

**Evidence:**
- Document major structural problems
- List missing organizational elements
- Note severe length issues

---

## Category 5: AI-First Teaching (10% weight)

### Excellent (90-100%)
**Learning WITH AI Philosophy:**
- Consistently emphasizes "learning WITH AI" not "generating FROM AI"
- Clear distinction: AI as learning mentor vs. code generator
- Students encouraged to write own code with AI as tutor
- AI used for understanding, explanation, exploration (not just answers)
- Philosophy explicitly taught and modeled throughout

**AI Exercise Quality:**
- 2-3 exercises specifically designed for AI-augmented learning
- Each exercise includes "Learning with AI" guidance section
- Example AI prompts provided that deepen understanding:
  - ✅ "Explain why this works"
  - ✅ "What does this error mean?"
  - ✅ "Compare these two approaches"
  - ❌ "Write this function for me"
  - ❌ "Fix my code"
- Exercises explicitly warn against using AI as code generator
- Students directed to: attempt → ask AI for clarity → understand → try again

**Responsible AI Use:**
- Modeling effective AI collaboration for learning throughout
- Shows how to ask good questions that lead to understanding
- Demonstrates iterative learning cycle: try → fail → learn with AI → understand
- Addresses AI limitations and when to rely on own thinking
- Teaches critical evaluation of AI suggestions

**Integration:**
- AI-augmented learning naturally integrated into content (not bolted on)
- AI learning prompts embedded throughout (not just in exercises)
- Clear pedagogical rationale for AI use in each instance
- AI tools mentioned are appropriate and accessible to learners

**Evidence:**
- Quote examples showing strong AI-first pedagogy
- Note exercises that exemplify learning WITH AI
- Cite integration of AI throughout content

---

### Good (75-89%)
**Learning WITH AI Philosophy:**
- Generally emphasizes "learning WITH AI"
- Distinction between AI as mentor vs. generator present
- Students mostly encouraged to write own code
- AI used for understanding and explanation
- Philosophy present but could be stronger

**AI Exercise Quality:**
- 1-2 exercises designed for AI-augmented learning
- Some "Learning with AI" guidance provided
- Some example AI prompts that deepen understanding
- Some warning against code generation
- Basic attempt → learn → try again cycle present

**Responsible AI Use:**
- Shows some effective AI collaboration techniques
- Some guidance on asking good questions
- Partial demonstration of iterative learning
- Some mention of AI limitations
- Some guidance on evaluating AI suggestions

**Integration:**
- AI-augmented learning mostly integrated
- Some AI learning prompts embedded
- Generally clear rationale for AI use
- AI tools mentioned are appropriate

**Evidence:**
- Note AI-first teaching elements present
- Identify areas for strengthening
- Confirm basic philosophy is clear

---

### Needs Work (50-74%)
**Learning WITH AI Philosophy:**
- Philosophy mentioned but not consistently emphasized
- Distinction between mentor and generator unclear
- Mixed messages about writing own code vs. using AI
- AI sometimes positioned as answer-provider rather than learning tool
- Philosophy present but weakly articulated

**AI Exercise Quality:**
- 0-1 exercises for AI-augmented learning
- Limited "Learning with AI" guidance
- Few or weak example AI prompts
- Insufficient warning against code generation, or warnings feel pro forma
- Iterative learning cycle partially present

**Responsible AI Use:**
- Limited modeling of effective collaboration
- Minimal guidance on asking questions
- Limited demonstration of learning cycle
- Little mention of AI limitations
- Weak guidance on evaluating suggestions

**Integration:**
- AI learning feels somewhat bolted on
- Few embedded AI prompts
- Rationale for AI use sometimes unclear
- AI tools mentioned may not all be accessible

**Evidence:**
- List gaps in AI-first teaching
- Note weak or missing AI exercises
- Identify unclear philosophy statements

---

### Insufficient (<50%)
**Learning WITH AI Philosophy:**
- Philosophy not articulated or missing
- No clear distinction between mentor and generator
- Encourages using AI to generate code without learning
- AI positioned as shortcut rather than learning tool
- Philosophy absent or contradictory to learning goals

**AI Exercise Quality:**
- No exercises for AI-augmented learning
- No "Learning with AI" guidance
- No example AI prompts, or prompts encourage code generation
- No warnings about appropriate AI use
- No iterative learning cycle

**Responsible AI Use:**
- No modeling of effective collaboration
- No guidance on asking questions
- No demonstration of learning cycle
- No mention of AI limitations
- No guidance on evaluating suggestions

**Integration:**
- AI learning not integrated or missing entirely
- No embedded AI prompts
- No rationale for AI use
- AI tools not mentioned or inappropriate

**Evidence:**
- Document missing AI-first teaching elements
- Note lack of appropriate AI exercises
- List ways content contradicts learning WITH AI philosophy

---

## Category 6: Constitution Compliance (Pass/Fail - GATE)

This category is evaluated as **Pass** or **Fail** only. If **Fail**, content cannot proceed regardless of weighted scores.

### Constitution Compliance Checklist

Use the detailed checklist in `references/constitution-checklist.md` to evaluate compliance with all constitutional principles and requirements.

**Key Areas:**
1. **Project Vision Alignment** - Content serves the stated project purpose and philosophy
2. **Core Principles Adherence** - All 11 core principles respected (if applicable to project)
3. **Domain Skills Application** - Required domain skills applied as specified
4. **Code Standards** - Code meets quality standards defined in constitution
5. **Quality Gates** - Content meets minimum quality thresholds
6. **Non-Negotiable Rules** - All "ALWAYS DO" and "NEVER DO" rules followed
7. **Output Style Compliance** - Content follows required output style template
8. **Accessibility Requirements** - Accessibility standards met (alt text, readability, etc.)
9. **Technical Requirements** - Technical specifications and dependencies satisfied
10. **Structural Requirements** - File organization and naming conventions followed

### Pass Criteria

Content receives **PASS** when:
- ALL critical non-negotiable rules are followed
- No violations of "NEVER DO" rules
- ALL "ALWAYS DO" rules are satisfied
- Required domain skills appropriately applied
- Minimum quality thresholds met
- Output style template followed
- All applicable constitutional principles respected

### Fail Criteria

Content receives **FAIL** if ANY of the following:
- Any critical non-negotiable rule violated
- Any "NEVER DO" rule violated
- Required "ALWAYS DO" rules not followed
- Required domain skills not applied or misapplied
- Minimum quality thresholds not met
- Output style template not followed
- Key constitutional principles violated

**If FAIL:** Content must be revised to address constitutional violations before proceeding with weighted evaluation.

---

## Using This Rubric

### Step-by-Step Process

1. **Read the content** being evaluated completely
2. **Review context** (spec, plan, learning objectives, constitution)
3. **Evaluate Constitution Compliance first** using checklist
   - If FAIL: Stop and report violations immediately
   - If PASS: Continue to weighted categories
4. **Evaluate each weighted category** using tier criteria above
5. **Record specific evidence** for each category score (quotes, line numbers, examples)
6. **Calculate weighted score** using formula
7. **Generate report** using evaluation template
8. **Provide actionable recommendations** prioritized by impact

### Tips for Consistent Evaluation

- **Be objective:** Base scores on rubric criteria, not subjective preference
- **Use evidence:** Always cite specific examples from content
- **Be fair:** Recognize strengths as well as weaknesses
- **Be constructive:** Frame criticism as opportunities for improvement
- **Be clear:** Provide concrete, actionable recommendations
- **Be consistent:** Apply same standards across all content units

---

**This rubric ensures rigorous, evidence-based, consistent quality evaluation for all educational content.**
